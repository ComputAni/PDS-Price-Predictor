{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from unicodedata import normalize\n",
    "import re, copy, random, time, csv, math\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from uberPrice import getPriceNow\n",
    "from create_weather_features import init_weather_features, create_weather_features\n",
    "from create_date_features import create_date_features\n",
    "from time import strftime, strptime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dropoff_latitude' 'dropoff_longitude' 'pickup_latitude'\n",
      " 'pickup_longitude' 'total_amount' 'tpep_dropoff_datetime'\n",
      " 'tpep_pickup_datetime' 'start_latlng' 'end_latlng' 'distance'\n",
      " 'tpep_dropoff_datetime_years' 'tpep_dropoff_datetime_months'\n",
      " 'tpep_dropoff_datetime_days' 'tpep_dropoff_datetime_hours'\n",
      " 'tpep_dropoff_datetime_minutes' 'tpep_dropoff_datetime_seconds'\n",
      " 'tpep_dropoff_datetime_day_of_week' 'tpep_dropoff_datetime_is_holiday'\n",
      " 'tpep_pickup_datetime_years' 'tpep_pickup_datetime_months'\n",
      " 'tpep_pickup_datetime_days' 'tpep_pickup_datetime_hours'\n",
      " 'tpep_pickup_datetime_minutes' 'tpep_pickup_datetime_seconds'\n",
      " 'tpep_pickup_datetime_day_of_week' 'tpep_pickup_datetime_is_holiday'\n",
      " 'duration' 'Mean Temperature' 'Max Temperature' 'Min Temperature'\n",
      " 'Dew Point' 'Average Humidity' 'Precipitation' 'Snow' 'Wind Speed'\n",
      " 'Visibility']\n"
     ]
    }
   ],
   "source": [
    "#Step 1, Read the dataset with features, clean any unecessary stuff\n",
    "cab_data = pd.read_csv(\"data_with_features.csv\")\n",
    "del cab_data[\"Unnamed: 0\"]\n",
    "del cab_data[\"Unnamed: 0.1\"]\n",
    "del cab_data[\"Unnamed: 0.1.1\"]\n",
    "#Print columns to validate\n",
    "print(cab_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  982936\n",
      "Data size:  978979\n"
     ]
    }
   ],
   "source": [
    "#Filter out any nonzero price values\n",
    "price_filter = cab_data[\"total_amount\"] > 0\n",
    "cab_data = cab_data[price_filter]\n",
    "\n",
    "print(\"Data size: \", len(cab_data))\n",
    "\n",
    "#Filter out any negative distance values\n",
    "dist_filter = cab_data['distance'] > 0\n",
    "cab_data = cab_data[dist_filter]\n",
    "\n",
    "#Add the log total amount (for price verification)\n",
    "cab_data[\"log_total_amount\"] = np.log(cab_data[\"total_amount\"] + 1)\n",
    "\n",
    "data_size = len(cab_data)\n",
    "print(\"Data size: \", data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    features = []\n",
    "    feature_names = ['distance', 'duration', 'tpep_dropoff_datetime_years', 'tpep_dropoff_datetime_months',\n",
    "       'tpep_dropoff_datetime_days', 'tpep_dropoff_datetime_hours',\n",
    "       'tpep_dropoff_datetime_minutes', 'tpep_dropoff_datetime_seconds',\n",
    "       'tpep_dropoff_datetime_day_of_week',\n",
    "       'tpep_dropoff_datetime_is_holiday', 'tpep_pickup_datetime_years',\n",
    "       'tpep_pickup_datetime_months', 'tpep_pickup_datetime_days',\n",
    "       'tpep_pickup_datetime_hours', 'tpep_pickup_datetime_minutes',\n",
    "       'tpep_pickup_datetime_seconds', 'tpep_pickup_datetime_day_of_week',\n",
    "       'tpep_pickup_datetime_is_holiday', 'Mean Temperature',\n",
    "       'Max Temperature', 'Min Temperature', 'Dew Point',\n",
    "       'Average Humidity', 'Precipitation', 'Snow', 'Wind Speed',\n",
    "       'Visibility']\n",
    "    for feature_name in feature_names:\n",
    "        features.append(df[feature_name])\n",
    "    \n",
    "    X = np.array(features).T\n",
    "    y = df[\"log_total_amount\"].values\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_test_split(X,y):\n",
    "    X_, X_test, y_, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)\n",
    "    X_train, X_cv, y_train, y_cv = train_test_split(X_, y_, test_size = 0.3, random_state=0)\n",
    "    \n",
    "    return (X_train, y_train, X_cv, y_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X,y) = create_features(cab_data)\n",
    "(X_train, y_train, X_cv, y_cv, X_test, y_test) = create_train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay great! The data is clean and is ready to be processed!\n",
    "\n",
    "The next step is to initialize the model and train on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Apply PCA\n",
    "def applyPCA(X_train, X_test, numComponents):\n",
    "    \n",
    "    # Initialize the PCA class and pass in the number of dimensions to which you want to reduce your data to.\n",
    "    pca = PCA(n_components = numComponents)\n",
    "\n",
    "    # Fit the training data to the PCA model.\n",
    "    pca.fit(X_train)\n",
    "\n",
    "    # Reduce the dimensionality of the training set.\n",
    "    newX_train = pca.transform(X_train)\n",
    "\n",
    "    # Reduce the dimensionality of the testing set.\n",
    "    newX_test = pca.transform(X_test)\n",
    "    \n",
    "    return newX_train, newX_test\n",
    "\n",
    "# Train the PCA and get the right number of \n",
    "def PCAWithLinearRegression(X_train, y_train, X_test, y_test):\n",
    "    numCols = X_train.shape[1]\n",
    "    errors = []\n",
    "    print(numCols)\n",
    "    for i in range(numCols):\n",
    "        print(i+1)\n",
    "        pcaX_train, pcaX_test = applyPCA(X_train, X_test, i+1)\n",
    "        (model, y_pred, error) = train_linear_regression(pcaX_train, y_train, pcaX_test, y_test)\n",
    "        errors.append(error)\n",
    "    return errors\n",
    "\n",
    "# Graphs above determine that 16 is the best number of components\n",
    "def PCAwithGradientBoostingAndLR(X_train, X_test, y_train, numComponents):\n",
    "    \n",
    "    pcaX_train, pcaX_test = applyPCA(X_train, X_test, numComponents)\n",
    "    clf = GradientBoostingRegressor()\n",
    "    clf.fit(pcaX_train, y_train)\n",
    "    modelResults = clf.predict(pcaX_test)\n",
    "    \n",
    "    return modelResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be making 72 calls to uber API.\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Determine interval for sampling Uber API\n",
    "time_period = 6*60 #In X hrs * 60 minutes \n",
    "sample_rate = 5 #In 1 sample per *sample_rate* minutes\n",
    "num_samples = time_period // sample_rate\n",
    "print(\"We will be making %d calls to uber API.\" % num_samples)\n",
    "\n",
    "#Step 2: Get random subset of data to use as test points\n",
    "sample_data = cab_data.sample(n=num_samples)\n",
    "sample_data = sample_data[['start_latlng', 'end_latlng', 'distance', 'duration']]\n",
    "#print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  27\n"
     ]
    }
   ],
   "source": [
    "#List of features we are using in our model\n",
    "feature_names = ['distance', 'duration', 'tpep_dropoff_datetime_years', 'tpep_dropoff_datetime_months',\n",
    "       'tpep_dropoff_datetime_days', 'tpep_dropoff_datetime_hours',\n",
    "       'tpep_dropoff_datetime_minutes', 'tpep_dropoff_datetime_seconds',\n",
    "       'tpep_dropoff_datetime_day_of_week',\n",
    "       'tpep_dropoff_datetime_is_holiday', 'tpep_pickup_datetime_years',\n",
    "       'tpep_pickup_datetime_months', 'tpep_pickup_datetime_days',\n",
    "       'tpep_pickup_datetime_hours', 'tpep_pickup_datetime_minutes',\n",
    "       'tpep_pickup_datetime_seconds', 'tpep_pickup_datetime_day_of_week',\n",
    "       'tpep_pickup_datetime_is_holiday', 'Mean Temperature',\n",
    "       'Max Temperature', 'Min Temperature', 'Dew Point',\n",
    "       'Average Humidity', 'Precipitation', 'Snow', 'Wind Speed',\n",
    "       'Visibility']\n",
    "print(\"Number of features: \", len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_estimates(sample_data):\n",
    "\n",
    "\n",
    "    data_with_features = []\n",
    "    uber_estimates = []\n",
    "\n",
    "    for (idx,row) in sample_data.iterrows():\n",
    "\n",
    "        ########## CREATE TIME/DATE/WEATHER FEATURES FOR MODEL ESTIMATE ###############\n",
    "        F = open('price_estimates.csv', 'a')\n",
    "        \n",
    "        #Pickup time is now, dropoff is trip duration after pickup\n",
    "        pickup_time = datetime.now()\n",
    "        dropoff_time = pickup_time + timedelta(minutes=row['duration'])\n",
    "\n",
    "        #Format start and end times for feature extraction\n",
    "        start = datetime(pickup_time.year, pickup_time.month, pickup_time.day)\n",
    "        start = '%4d-%02d-%02d' % (start.year, start.month, start.day)\n",
    "        end = datetime(pickup_time.year, pickup_time.month, pickup_time.day + 1)\n",
    "        end = '%4d-%02d-%02d' % (end.year, end.month, end.day)\n",
    "        #print(start,end, type(start), type(end))\n",
    "\n",
    "        start_date = datetime(pickup_time.year, pickup_time.month, pickup_time.day)\n",
    "        end_date = datetime(pickup_time.year, pickup_time.month, pickup_time.day)\n",
    "        #print(start_date, end_date)\n",
    "        weather_date_features = init_weather_features(start_date, end_date)\n",
    "\n",
    "        #Format pickup/dropoff time\n",
    "        pickup_time = '%4d-%02d-%02dT%02d:%02d:%f' % (pickup_time.year, pickup_time.month, pickup_time.day, pickup_time.hour, pickup_time.minute, pickup_time.second)\n",
    "        dropoff_time = '%4d-%02d-%02dT%02d:%02d:%f' % (dropoff_time.year, dropoff_time.month, dropoff_time.day, dropoff_time.hour, dropoff_time.minute, dropoff_time.second)\n",
    "        #print(pickup_time, dropoff_time)\n",
    "        curr_row = dict()\n",
    "        curr_row[\"tpep_dropoff_datetime\"] = str(dropoff_time)[:len(dropoff_time)-3] #strftime('%Y-%m-%dT%H:%M:%S.%f', dropoff_time)\n",
    "        curr_row[\"tpep_pickup_datetime\"] = str(pickup_time)[:len(pickup_time)-3] #strftime('%Y-%m-%dT%H:%M:%S.%f', pickup_time)\n",
    "\n",
    "        curr_row['distance'] = row['distance']\n",
    "        \n",
    "        #Convert to dataframe and get date  + weather features\n",
    "        curr_row = pd.DataFrame([curr_row])\n",
    "        curr_row = create_date_features(curr_row, start, end)\n",
    "        curr_row = create_weather_features(curr_row, weather_date_features)\n",
    "\n",
    "        #Write the synthesized row of features to csv file (append)\n",
    "        curr_row.to_csv(\"price_features.csv\", mode='a',header=False)\n",
    "        data_with_features.append(curr_row)\n",
    "        \n",
    "        #print(curr_row, curr_row['distance'])\n",
    "\n",
    "\n",
    "        ######### GET UBER ESTIMATE ##############\n",
    "        start_coord = row['start_latlng'].split(',')\n",
    "        end_coord = row['end_latlng'].split(',')\n",
    "        #print(start_coord, end_coord, type(start_coord))\n",
    "        curr_uber_estimates = getPriceNow(float(start_coord[0]), float(start_coord[1]), float(end_coord[0]), float(end_coord[1]))\n",
    "        uberX_estimate = curr_uber_estimates[1]\n",
    "\n",
    "        high_estimate = uberX_estimate['high_estimate']\n",
    "        low_estimate = uberX_estimate['low_estimate']\n",
    "        avg_estimate = (high_estimate + low_estimate) / 2\n",
    "\n",
    "        all_estimates = (high_estimate, low_estimate, avg_estimate)\n",
    "        \n",
    "        uber_estimates.append(all_estimates)\n",
    "\n",
    "        print(\"uberX estimate: %s\" % (avg_estimate))\n",
    "        F.write(str(all_estimates) + \"\\n\")\n",
    "        \n",
    "        ####### COMPARE UBER ESTIMATE WITH MODEL ESTIMATE #########\n",
    "        F.close()\n",
    "        time.sleep(sample_rate * 60)\n",
    "        #time.sleep(1)\n",
    "    \n",
    "\n",
    "    \n",
    "    return (data_with_features, uber_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 5 6\n",
      "uberX estimate: 12.5\n"
     ]
    }
   ],
   "source": [
    "features, estimates = generate_estimates(sample_data)\n",
    "#print(features)\n",
    "#print(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check model performance\n",
    "\n",
    "####### STEP 1: Get model prediction of price (from our synthesized features file) ########\n",
    "X_test = pd.read_csv('price_features.csv')\n",
    "del X_test['i']\n",
    "del X_test['tpep_dropoff_datetime']\n",
    "del X_test['tpep_pickup_datetime']\n",
    "#print(list(X_test), len(list(X_test)))\n",
    "modelResults = PCAwithGradientBoostingAndLR(X_train, X_test, y_train, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.02160475  1.02160475  2.33850251]\n",
      "[1.7776486169694596, 1.7776486169694596, 9.365702427436569]\n"
     ]
    }
   ],
   "source": [
    "print(modelResults)\n",
    "model_results = [math.exp(r)-1 for r in modelResults]\n",
    "print(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['(17.0', ' 13.0', ' 15.0)'], ['(23.0', ' 18.0', ' 20.5)'], ['(69.0', ' 55.0', ' 62.0)']]\n"
     ]
    }
   ],
   "source": [
    "####### STEP 2: Get Uber prediction of price (already written in file)\n",
    "uber_pricing_estimates = []\n",
    "with open('price_estimates.csv', 'r') as f:\n",
    "    r = csv.reader(f)\n",
    "    uber_pricing_estimates = list(r)\n",
    "\n",
    "uber_pricing_estimates.pop(0)\n",
    "print(uber_pricing_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For trip 0, Model: 1.777649        Uber:  15.0\n",
      "For trip 1, Model: 1.777649        Uber:  20.5\n",
      "For trip 2, Model: 9.365702        Uber:  62.0\n"
     ]
    }
   ],
   "source": [
    "assert(len(model_results) == len(uber_pricing_estimates))\n",
    "\n",
    "for i in range(len(model_results)):\n",
    "    uber_hi, uber_lo, uber_avg = uber_pricing_estimates[i]\n",
    "    #Weird formatting stuff, ignore for now\n",
    "    uber_avg = uber_avg[:len(uber_avg)-1]\n",
    "    uber_hi = uber_hi[1:]\n",
    "\n",
    "    our_estimate = model_results[i]\n",
    "    \n",
    "    print(\"For trip %d, Model: %f        Uber: %s\" % (i, our_estimate, uber_avg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
